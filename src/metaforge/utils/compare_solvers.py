import os
import time
import csv
import copy  # === å…³é”®ç‚¹2ï¼šå¯¼å…¥ copy æ¨¡å— ===
import matplotlib.pyplot as plt
from sympy import false

from metaforge.problems.benchmark_loader import load_job_shop_instance
from metaforge.utils.timer import Timer
from metaforge.metaforge_runner import run_solver
from metaforge.utils.plotting import plot_solver_comparison
# === å…³é”®ç‚¹3ï¼šå¯¼å…¥æˆ‘ä»¬æ–°çš„â€œä¸»â€ç»˜å›¾å‡½æ•°ï¼Œæ›¿æ¢æ—§çš„ ===
from metaforge.utils.visualization import plot_comprehensive_comparison
from metaforge.utils.pretty_names import pretty_names # pretty_names åœ¨ç»˜å›¾å‡½æ•°å†…éƒ¨åŠ è½½ï¼Œè¿™é‡Œä¸éœ€è¦


def compare_solvers(solver_names, problem, track_schedule=True, plot=False, dynamic_events=None):
    """
    è¿è¡Œå¹¶æ¯”è¾ƒå¤šä¸ªæ±‚è§£å™¨ï¼Œå¹¶åœ¨éœ€è¦æ—¶è‡ªåŠ¨ç”Ÿæˆä¸€å¥—å®Œæ•´çš„å¯¹æ¯”å›¾è¡¨ã€‚
    """
    results = {}

    # å­˜å‚¨åˆå§‹å·¥ä»¶æ•°é‡ï¼Œä»¥ä¾¿ä¹‹åä¼ é€’ç»™ç»˜å›¾å‡½æ•°
    initial_num_jobs = len(problem.jobs)

    for solver in solver_names:
        print(f"ğŸ”§ Running solver: {solver}...")

        # === å…³é”®ç‚¹2ï¼šä¸ºæ¯ä¸ªæ±‚è§£å™¨åˆ›å»ºé—®é¢˜çš„æ·±æ‹·è´ï¼Œä¿è¯å…¬å¹³ ===
        problem_copy = copy.deepcopy(problem)
        events_copy = copy.deepcopy(dynamic_events) if dynamic_events else None

        start = time.time()
        # === å…³é”®ç‚¹1ï¼šå°† dynamic_events ä¼ é€’ç»™ run_solver ===
        output = run_solver(
            solver,
            problem_copy,
            track_schedule=track_schedule,
            dynamic_events=events_copy

        )
        end = time.time()
        # --- ã€æ ¸å¿ƒä¿®å¤ã€‘ ---
        # åœ¨å°† run_solver çš„è¾“å‡ºå­˜å…¥æœ€ç»ˆçš„ results å­—å…¸æ—¶ï¼Œ
        # å¯¹ schedules åˆ—è¡¨è¿›è¡Œæ·±æ‹·è´ï¼Œä»¥ç¡®ä¿å…¶å®Œå…¨ç‹¬ç«‹å’Œå®‰å…¨ã€‚
        schedules_copy = copy.deepcopy(output.get("schedules"))
        results[solver] = {
            "best_score": output["makespan"],
            "runtime_sec": round(end - start, 2),
            "best_solution": output.get("solution"),
            "all_schedules": output.get("schedules"),
            "history": output.get("history")
        }
    if plot:
        plot_solver_comparison(results)
    # === å…³é”®ç‚¹3ï¼šå½“ plot=True æ—¶ï¼Œè°ƒç”¨æˆ‘ä»¬å‡çº§åçš„â€œä¸»â€ç»˜å›¾å‡½æ•° ===
    elif plot == False:
        # åŠ¨æ€åŠ è½½ pretty_names ä»¥é¿å…åœ¨é¡¶å±‚å¯¼å…¥æ—¶å¯èƒ½äº§ç”Ÿçš„å¾ªç¯ä¾èµ–é—®é¢˜
        from metaforge.utils.pretty_names import pretty_names
        plot_solver_comparison(results)
        # è°ƒç”¨è¿™ä¸ªå‡½æ•°ï¼Œå®ƒä¼šè‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰ä¸‰ä¸ªå›¾è¡¨
        plot_comprehensive_comparison(
            results=results,
            problem=problem,  # ä¼ é€’åŸå§‹problemä»¥è·å–æœºå™¨æ•°ç­‰ä¿¡æ¯
            pretty_names=pretty_names,
            dynamic_events=dynamic_events,
            initial_num_jobs=initial_num_jobs
        )

    return results


def compare_all_benchmarks(
    benchmark_source,
    solvers,
    format="orlib",
    output_csv="results/benchmark_comparison.csv",
    track_schedule=False,
    plot=False
):
    # Determine if source is a URL or local path
    is_url = benchmark_source.startswith("http://") or benchmark_source.startswith("https://")

    if output_csv:
        output_dir = os.path.dirname(output_csv)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)

    # Define benchmark files (common ORLib ones by default)
    benchmark_files = [
        "ft06.txt", "ft10.txt", "ft20.txt",
        "la01.txt", "la02.txt", "la03.txt",
        "la04.txt", "la05.txt"
    ]

    results = []

    for benchmark_file in benchmark_files:
        path = (
            f"{benchmark_source.rstrip('/')}/{benchmark_file}" if is_url
            else os.path.join(benchmark_source, benchmark_file)
        )

        try:
            problem = load_job_shop_instance(path, format=format)
        except Exception as e:
            print(f"âš ï¸ Failed to load {benchmark_file}: {e}")
            continue

        for solver in solvers:
            solver_label = pretty_names.get(solver, solver)
            print(f"Running {solver_label} on {benchmark_file}...")

            timer = Timer()
            result = run_solver(
                solver,
                problem,
                track_schedule=track_schedule
            )
            elapsed = timer.stop()

            results.append({
                "benchmark": benchmark_file,
                "solver": solver_label,
                "best_score": result["makespan"],
                "runtime_sec": elapsed,
                "best_solution": result["solution"],
                "all_schedules": result["schedules"],
                "history": result["history"],
            })

    # Write summary results to CSV
    if output_csv:
        with open(output_csv, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=["benchmark", "solver", "best_score", "runtime_sec"])
            writer.writeheader()
            for row in results:
                writer.writerow({
                    "benchmark": row["benchmark"],
                    "solver": row["solver"],
                    "best_score": row["best_score"],
                    "runtime_sec": row["runtime_sec"],
                })
        print(f"\nâœ… All results saved to {output_csv}")

    # Optional plotting
    if plot:
        from metaforge.utils.visualization import (
            plot_results_from_csv,
            plot_runtime_from_csv,
        )
        plot_results_from_csv(output_csv)
        plot_runtime_from_csv(output_csv)

    return results
